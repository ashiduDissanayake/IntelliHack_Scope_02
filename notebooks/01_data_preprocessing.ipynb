{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation - Data Preprocessing\n",
    "\n",
    "This notebook covers the data preprocessing steps for the customer segmentation project:\n",
    "1. Loading the data\n",
    "2. Exploring the raw data\n",
    "3. Handling missing values\n",
    "4. Detecting and addressing outliers\n",
    "5. Feature scaling/normalization\n",
    "6. Saving the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set plotting style\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Increase default figure size\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the customer behavior data\n",
    "file_path = \"../data/customer_behavior_analytcis.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and basic information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistical summary\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_info = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Missing Percentage': missing_percentage.round(2)\n",
    "})\n",
    "\n",
    "# Display columns with missing values\n",
    "missing_info[missing_info['Missing Values'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataframe\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Check for rows with multiple missing values\n",
    "rows_with_multiple_missing = df_clean[df_clean.isnull().sum(axis=1) > 1]\n",
    "print(f\"Number of rows with multiple missing values: {len(rows_with_multiple_missing)}\")\n",
    "\n",
    "# Drop rows with too many missing values (if needed)\n",
    "if len(rows_with_multiple_missing) > 0:\n",
    "    df_clean = df_clean.dropna(thresh=df_clean.shape[1]-1)\n",
    "    print(f\"After dropping rows with multiple missing values: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill remaining missing values with median/mode\n",
    "for column in df_clean.columns:\n",
    "    if df_clean[column].isnull().sum() > 0:\n",
    "        if pd.api.types.is_numeric_dtype(df_clean[column]):\n",
    "            median_value = df_clean[column].median()\n",
    "            df_clean[column].fillna(median_value, inplace=True)\n",
    "            print(f\"Filled missing values in '{column}' with median: {median_value}\")\n",
    "        else:\n",
    "            mode_value = df_clean[column].mode()[0]\n",
    "            df_clean[column].fillna(mode_value, inplace=True)\n",
    "            print(f\"Filled missing values in '{column}' with mode: {mode_value}\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"\\nRemaining missing values: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detecting and Addressing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots to visualize potential outliers\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Select numeric columns excluding customer_id\n",
    "numeric_columns = [col for col in df_clean.columns \n",
    "                   if pd.api.types.is_numeric_dtype(df_clean[col]) and col != 'customer_id']\n",
    "\n",
    "for i, column in enumerate(numeric_columns):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.boxplot(x=df_clean[column])\n",
    "    plt.title(f'Box Plot of {column}')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check outliers in each feature\n",
    "for column in numeric_columns:\n",
    "    outliers, lower_bound, upper_bound = detect_outliers(df_clean, column)\n",
    "    n_outliers = len(outliers)\n",
    "    print(f\"{column}: {n_outliers} outliers detected ({n_outliers/len(df_clean)*100:.2f}%)\")\n",
    "    print(f\"    Bounds: ({lower_bound:.2f}, {upper_bound:.2f})\")\n",
    "    print(f\"    Min: {df_clean[column].min()}, Max: {df_clean[column].max()}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this segmentation problem, we'll keep the outliers\n",
    "# They likely represent valid customer behaviors that are important for segmentation\n",
    "print(\"Note: For customer segmentation, outliers often represent important customer behaviors.\")\n",
    "print(\"We'll keep outliers in the dataset as they may help identify distinct segments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling/Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the original dataframe before scaling\n",
    "df_original = df_clean.copy()\n",
    "\n",
    "# Identify columns to normalize (excluding customer_id if present)\n",
    "columns_to_normalize = [col for col in df_clean.columns \n",
    "                        if pd.api.types.is_numeric_dtype(df_clean[col]) and col != 'customer_id']\n",
    "\n",
    "# Create a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling to the selected columns\n",
    "df_clean[columns_to_normalize] = scaler.fit_transform(df_clean[columns_to_normalize])\n",
    "\n",
    "# Display the scaled data\n",
    "print(\"Data after scaling:\")\n",
    "df_clean[columns_to_normalize].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of scaled features\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for i, column in enumerate(columns_to_normalize):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.histplot(df_clean[column], kde=True)\n",
    "    plt.title(f'Distribution of {column} (Scaled)')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../output', exist_ok=True)\n",
    "\n",
    "# Save the preprocessed data\n",
    "df_clean.to_csv('../output/preprocessed_data.csv', index=False)\n",
    "df_original.to_csv('../output/cleaned_data_unscaled.csv', index=False)\n",
    "\n",
    "print(f\"Preprocessed data saved to '../output/preprocessed_data.csv'\")\n",
    "print(f\"Cleaned unscaled data saved to '../output/cleaned_data_unscaled.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've completed the following preprocessing steps:\n",
    "\n",
    "1. Loaded the customer behavior data\n",
    "2. Explored the dataset's basic properties\n",
    "3. Identified and handled missing values\n",
    "4. Detected outliers (but kept them for segmentation purposes)\n",
    "5. Normalized the features using StandardScaler\n",
    "6. Saved both the preprocessed data and the cleaned unscaled data\n",
    "\n",
    "The preprocessed data is now ready for exploratory data analysis and customer segmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
