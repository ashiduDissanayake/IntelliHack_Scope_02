{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation - Model Building\n",
    "\n",
    "This notebook focuses on building and evaluating clustering models to identify customer segments:\n",
    "\n",
    "1. Find the optimal number of clusters\n",
    "2. Apply different clustering algorithms\n",
    "3. Compare clustering results\n",
    "4. Select the best model\n",
    "5. Visualize the identified clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Increase default figure size\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('./output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "preprocessed_file = './output/preprocessed_data.csv'\n",
    "df = pd.read_csv(preprocessed_file)\n",
    "\n",
    "# Also load the unscaled data for interpretation\n",
    "unscaled_file = './output/cleaned_data_unscaled.csv'\n",
    "df_unscaled = pd.read_csv(unscaled_file)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "# Exclude customer_id from clustering if present\n",
    "if 'customer_id' in df.columns:\n",
    "    features_df = df.drop(columns=['customer_id'])\n",
    "else:\n",
    "    features_df = df.copy()\n",
    "\n",
    "feature_names = features_df.columns.tolist()\n",
    "print(f\"Features used for clustering: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finding the Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the optimal number of clusters\n",
    "def find_optimal_k(data, max_clusters=10):\n",
    "    # Initialize empty lists to store results\n",
    "    inertia = []\n",
    "    silhouette = []\n",
    "    calinski_harabasz = []\n",
    "    davies_bouldin = []\n",
    "    \n",
    "    # Range of clusters to try\n",
    "    K = range(2, max_clusters+1)\n",
    "    \n",
    "    for k in K:\n",
    "        # KMeans clustering\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(data)\n",
    "        \n",
    "        # Get cluster labels\n",
    "        labels = kmeans.labels_\n",
    "        \n",
    "        # Inertia (within-cluster sum-of-squares)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        \n",
    "        # Silhouette score\n",
    "        silhouette.append(silhouette_score(data, labels))\n",
    "        \n",
    "        # Calinski-Harabasz Index\n",
    "        calinski_harabasz.append(calinski_harabasz_score(data, labels))\n",
    "        \n",
    "        # Davies-Bouldin Index\n",
    "        davies_bouldin.append(davies_bouldin_score(data, labels))\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    results = {\n",
    "        'k_values': list(K),\n",
    "        'inertia': inertia,\n",
    "        'silhouette': silhouette,\n",
    "        'calinski_harabasz': calinski_harabasz,\n",
    "        'davies_bouldin': davies_bouldin\n",
    "    }\n",
    "    \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k\n",
    "results = find_optimal_k(features_df, max_clusters=10)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot Inertia (Elbow Method)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(results['k_values'], results['inertia'], 'bo-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Silhouette Score\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(results['k_values'], results['silhouette'], 'go-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score (higher is better)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Calinski-Harabasz Index\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(results['k_values'], results['calinski_harabasz'], 'ro-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "plt.title('Calinski-Harabasz Index (higher is better)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Davies-Bouldin Index\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(results['k_values'], results['davies_bouldin'], 'mo-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.title('Davies-Bouldin Index (lower is better)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/optimal_k_metrics.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use yellowbrick for elbow visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=42), k=(2, 10), timings=False)\n",
    "visualizer.fit(features_df)\n",
    "visualizer.finalize()\n",
    "plt.savefig('./output/elbow_visualizer.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use yellowbrick for silhouette visualization for k=3 (based on domain knowledge)\n",
    "plt.figure(figsize=(10, 6))\n",
    "model = KMeans(n_clusters=3, random_state=42)\n",
    "visualizer = SilhouetteVisualizer(model, colors='yellowbrick')\n",
    "visualizer.fit(features_df)\n",
    "visualizer.finalize()\n",
    "plt.title('Silhouette Plot for KMeans with 3 clusters')\n",
    "plt.savefig('./output/silhouette_k3.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimal number of clusters based on different metrics\n",
    "k_values = np.array(results['k_values'])\n",
    "silhouette_scores = np.array(results['silhouette'])\n",
    "ch_scores = np.array(results['calinski_harabasz'])\n",
    "db_scores = np.array(results['davies_bouldin'])\n",
    "\n",
    "print(f\"Optimal k based on Silhouette Score: {k_values[np.argmax(silhouette_scores)]} (score: {np.max(silhouette_scores):.4f})\")\n",
    "print(f\"Optimal k based on Calinski-Harabasz Index: {k_values[np.argmax(ch_scores)]} (score: {np.max(ch_scores):.4f})\")\n",
    "print(f\"Optimal k based on Davies-Bouldin Index: {k_values[np.argmin(db_scores)]} (score: {np.min(db_scores):.4f})\")\n",
    "print(\"\\nNote: Based on domain knowledge, we expect 3 customer segments (Bargain Hunters, High Spenders, Window Shoppers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Different Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use k=3 based on domain knowledge (we expect 3 customer segments)\n",
    "n_clusters = 3\n",
    "\n",
    "# Dictionary to store clustering results\n",
    "clustering_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. K-Means clustering\n",
    "print(\"\\nApplying K-Means clustering...\")\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "kmeans.fit(features_df)\n",
    "kmeans_labels = kmeans.labels_\n",
    "kmeans_centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "kmeans_silhouette = silhouette_score(features_df, kmeans_labels)\n",
    "kmeans_calinski = calinski_harabasz_score(features_df, kmeans_labels)\n",
    "kmeans_davies = davies_bouldin_score(features_df, kmeans_labels)\n",
    "\n",
    "print(f\"K-Means Silhouette Score: {kmeans_silhouette:.4f}\")\n",
    "print(f\"K-Means Calinski-Harabasz Index: {kmeans_calinski:.4f}\")\n",
    "print(f\"K-Means Davies-Bouldin Index: {kmeans_davies:.4f}\")\n",
    "\n",
    "# Store the results\n",
    "clustering_results['kmeans'] = {\n",
    "    'model': kmeans,\n",
    "    'labels': kmeans_labels,\n",
    "    'centroids': kmeans_centroids,\n",
    "    'silhouette': kmeans_silhouette,\n",
    "    'calinski_harabasz': kmeans_calinski,\n",
    "    'davies_bouldin': kmeans_davies\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Gaussian Mixture Model (GMM)\n",
    "print(\"\\nApplying Gaussian Mixture Model...\")\n",
    "gmm = GaussianMixture(n_components=n_clusters, random_state=42, n_init=10)\n",
    "gmm.fit(features_df)\n",
    "gmm_labels = gmm.predict(features_df)\n",
    "gmm_centroids = gmm.means_\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "gmm_silhouette = silhouette_score(features_df, gmm_labels)\n",
    "gmm_calinski = calinski_harabasz_score(features_df, gmm_labels)\n",
    "gmm_davies = davies_bouldin_score(features_df, gmm_labels)\n",
    "\n",
    "print(f\"GMM Silhouette Score: {gmm_silhouette:.4f}\")\n",
    "print(f\"GMM Calinski-Harabasz Index: {gmm_calinski:.4f}\")\n",
    "print(f\"GMM Davies-Bouldin Index: {gmm_davies:.4f}\")\n",
    "\n",
    "# Store the results\n",
    "clustering_results['gmm'] = {\n",
    "    'model': gmm,\n",
    "    'labels': gmm_labels,\n",
    "    'centroids': gmm_centroids,\n",
    "    'silhouette': gmm_silhouette,\n",
    "    'calinski_harabasz': gmm_calinski,\n",
    "    'davies_bouldin': gmm_davies\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Hierarchical Clustering\n",
    "print(\"\\nApplying Hierarchical Clustering...\")\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hierarchical_labels = hierarchical.fit_predict(features_df)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "hierarchical_silhouette = silhouette_score(features_df, hierarchical_labels)\n",
    "hierarchical_calinski = calinski_harabasz_score(features_df, hierarchical_labels)\n",
    "hierarchical_davies = davies_bouldin_score(features_df, hierarchical_labels)\n",
    "\n",
    "print(f\"Hierarchical Silhouette Score: {hierarchical_silhouette:.4f}\")\n",
    "print(f\"Hierarchical Calinski-Harabasz Index: {hierarchical_calinski:.4f}\")\n",
    "print(f\"Hierarchical Davies-Bouldin Index: {hierarchical_davies:.4f}\")\n",
    "\n",
    "# Store the results\n",
    "clustering_results['hierarchical'] = {\n",
    "    'model': hierarchical,\n",
    "    'labels': hierarchical_labels,\n",
    "    'silhouette': hierarchical_silhouette,\n",
    "    'calinski_harabasz': hierarchical_calinski,\n",
    "    'davies_bouldin': hierarchical_davies\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DBSCAN\n",
    "print(\"\\nApplying DBSCAN...\")\n",
    "# DBSCAN requires careful parameter tuning\n",
    "# We'll use a simple heuristic for eps (the maximum distance between samples)\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Determine eps value using k-distance graph (k=min_samples-1)\n",
    "k = 5  # min_samples will be k+1 = 6\n",
    "neigh = NearestNeighbors(n_neighbors=k)\n",
    "neigh.fit(features_df)\n",
    "distances, indices = neigh.kneighbors(features_df)\n",
    "distances = np.sort(distances[:, -1])\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Data Points (sorted by distance)')\n",
    "plt.ylabel(f'{k}-th Nearest Neighbor Distance')\n",
    "plt.title('K-Distance Graph for DBSCAN Parameter Selection')\n",
    "plt.grid(True)\n",
    "plt.savefig('./output/dbscan_kdistance.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Choose eps where the curve shows an \"elbow\"\n",
    "# This is a visual inspection, but you can also automate this\n",
    "eps = 0.5  # This value should be adjusted based on the k-distance graph\n",
    "min_samples = k + 1\n",
    "\n",
    "print(f\"Using eps={eps} and min_samples={min_samples} for DBSCAN\")\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "dbscan_labels = dbscan.fit_predict(features_df)\n",
    "\n",
    "# Count the number of clusters and noise points\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "print(f\"DBSCAN found {n_clusters_dbscan} clusters and {n_noise} noise points\")\n",
    "\n",
    "# Calculate evaluation metrics only if we have multiple clusters and no noise points\n",
    "# or if we can exclude noise points\n",
    "if n_clusters_dbscan > 1:\n",
    "    if -1 not in dbscan_labels:\n",
    "        dbscan_silhouette = silhouette_score(features_df, dbscan_labels)\n",
    "        dbscan_calinski = calinski_harabasz_score(features_df, dbscan_labels)\n",
    "        dbscan_davies = davies_bouldin_score(features_df, dbscan_labels)\n",
    "    else:\n",
    "        # Exclude noise points for evaluation\n",
    "        mask = dbscan_labels != -1\n",
    "        if sum(mask) > 1:  # Ensure we have at least 2 non-noise points\n",
    "            dbscan_silhouette = silhouette_score(features_df[mask], dbscan_labels[mask])\n",
    "            dbscan_calinski = calinski_harabasz_score(features_df[mask], dbscan_labels[mask])\n",
    "            dbscan_davies = davies_bouldin_score(features_df[mask], dbscan_labels[mask])\n",
    "        else:\n",
    "            dbscan_silhouette = float('nan')\n",
    "            dbscan_calinski = float('nan')\n",
    "            dbscan_davies = float('nan')\n",
    "else:\n",
    "    dbscan_silhouette = float('nan')\n",
    "    dbscan_calinski = float('nan')\n",
    "    dbscan_davies = float('nan')\n",
    "\n",
    "print(f\"DBSCAN Silhouette Score: {dbscan_silhouette:.4f}\")\n",
    "print(f\"DBSCAN Calinski-Harabasz Index: {dbscan_calinski:.4f}\")\n",
    "print(f\"DBSCAN Davies-Bouldin Index: {dbscan_davies:.4f}\")\n",
    "\n",
    "# Store the results\n",
    "clustering_results['dbscan'] = {\n",
    "    'model': dbscan,\n",
    "    'labels': dbscan_labels,\n",
    "    'n_clusters': n_clusters_dbscan,\n",
    "    'n_noise': n_noise,\n",
    "    'silhouette': dbscan_silhouette,\n",
    "    'calinski_harabasz': dbscan_calinski,\n",
    "    'davies_bouldin': dbscan_davies\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance metrics of different clustering algorithms\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Algorithm': ['K-Means', 'GMM', 'Hierarchical', 'DBSCAN'],\n",
    "    'Silhouette Score': [\n",
    "        clustering_results['kmeans']['silhouette'],\n",
    "        clustering_results['gmm']['silhouette'],\n",
    "        clustering_results['hierarchical']['silhouette'],\n",
    "        clustering_results['dbscan']['silhouette']\n",
    "    ],\n",
    "    'Calinski-Harabasz Index': [\n",
    "        clustering_results['kmeans']['calinski_harabasz'],\n",
    "        clustering_results['gmm']['calinski_harabasz'],\n",
    "        clustering_results['hierarchical']['calinski_harabasz'],\n",
    "        clustering_results['dbscan']['calinski_harabasz']\n",
    "    ],\n",
    "    'Davies-Bouldin Index': [\n",
    "        clustering_results['kmeans']['davies_bouldin'],\n",
    "        clustering_results['gmm']['davies_bouldin'],\n",
    "        clustering_results['hierarchical']['davies_bouldin'],\n",
    "        clustering_results['dbscan']['davies_bouldin']\n",
    "    ]\n",
    "})\n",
    "\n",
    "metrics_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Silhouette Score (higher is better)\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.barplot(x='Algorithm', y='Silhouette Score', data=metrics_comparison)\n",
    "plt.title('Silhouette Score Comparison (Higher is Better)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Calinski-Harabasz Index (higher is better)\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.barplot(x='Algorithm', y='Calinski-Harabasz Index', data=metrics_comparison)\n",
    "plt.title('Calinski-Harabasz Index Comparison (Higher is Better)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Davies-Bouldin Index (lower is better)\n",
    "plt.subplot(3, 1, 3)\n",
    "sns.barplot(x='Algorithm', y='Davies-Bouldin Index', data=metrics_comparison)\n",
    "plt.title('Davies-Bouldin Index Comparison (Lower is Better)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/clustering_comparison.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on Silhouette Score\n",
    "silhouette_scores = [clustering_results[alg]['silhouette'] for alg in ['kmeans', 'gmm', 'hierarchical', 'dbscan']]\n",
    "best_algorithm_idx = np.nanargmax(silhouette_scores)  # Using nanargmax to handle NaN values\n",
    "best_algorithm = ['kmeans', 'gmm', 'hierarchical', 'dbscan'][best_algorithm_idx]\n",
    "\n",
    "print(f\"Best clustering algorithm based on Silhouette Score: {best_algorithm}\")\n",
    "print(f\"Silhouette Score: {clustering_results[best_algorithm]['silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we know there are 3 customer segments, we'll use the model that performs best \n",
    "# with n_clusters=3 and has the highest Silhouette Score among K-Means, GMM, and Hierarchical\n",
    "silhouette_3_clusters = [clustering_results[alg]['silhouette'] for alg in ['kmeans', 'gmm', 'hierarchical']]\n",
    "best_3_cluster_idx = np.argmax(silhouette_3_clusters)\n",
    "best_3_cluster_algorithm = ['kmeans', 'gmm', 'hierarchical'][best_3_cluster_idx]\n",
    "\n",
    "print(f\"Best algorithm for 3 clusters: {best_3_cluster_algorithm}\")\n",
    "print(f\"Silhouette Score: {clustering_results[best_3_cluster_algorithm]['silhouette']:.4f}\")\n",
    "\n",
    "# We'll use this as our final model\n",
    "final_model = clustering_results[best_3_cluster_algorithm]['model']\n",
    "final_labels = clustering_results[best_3_cluster_algorithm]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the Identified Clusters\n",
    "\n",
    "Continuing from the previous notebook, we'll visualize the clusters identified by our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize clusters in 2D using PCA\n",
    "def visualize_clusters_2d(data, labels, title=\"Cluster Visualization\", save_path=None):\n",
    "    # Apply PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(data)\n",
    "    \n",
    "    # Create a dataframe with principal components and cluster labels\n",
    "    pca_df = pd.DataFrame(\n",
    "        data=principal_components,\n",
    "        columns=['PC1', 'PC2']\n",
    "    )\n",
    "    pca_df['Cluster'] = labels\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df, palette='viridis', s=80, alpha=0.8)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
    "    plt.grid(linestyle='--', alpha=0.5)\n",
    "    plt.legend(title='Cluster', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Create interactive scatter plot using plotly\n",
    "    fig = px.scatter(\n",
    "        pca_df, \n",
    "        x='PC1', \n",
    "        y='PC2', \n",
    "        color='Cluster',\n",
    "        title=title,\n",
    "        labels={\n",
    "            'PC1': f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)',\n",
    "            'PC2': f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)'\n",
    "        }\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=10))\n",
    "    \n",
    "    if save_path:\n",
    "        html_path = save_path.replace('.png', '.html')\n",
    "        fig.write_html(html_path)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return pca_df, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize clusters in 3D using PCA\n",
    "def visualize_clusters_3d(data, labels, title=\"3D Cluster Visualization\", save_path=None):\n",
    "    # Apply PCA for visualization\n",
    "    pca = PCA(n_components=3)\n",
    "    principal_components = pca.fit_transform(data)\n",
    "    \n",
    "    # Create a dataframe with principal components and cluster labels\n",
    "    pca_df = pd.DataFrame(\n",
    "        data=principal_components,\n",
    "        columns=['PC1', 'PC2', 'PC3']\n",
    "    )\n",
    "    pca_df['Cluster'] = labels\n",
    "    \n",
    "    # Create interactive 3D scatter plot\n",
    "    fig = px.scatter_3d(\n",
    "        pca_df, \n",
    "        x='PC1', \n",
    "        y='PC2', \n",
    "        z='PC3',\n",
    "        color='Cluster',\n",
    "        title=title,\n",
    "        labels={\n",
    "            'PC1': f'PC1 ({pca.explained_variance_ratio_[0]:.2%})',\n",
    "            'PC2': f'PC2 ({pca.explained_variance_ratio_[1]:.2%})',\n",
    "            'PC3': f'PC3 ({pca.explained_variance_ratio_[2]:.2%})'\n",
    "        }\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=5))\n",
    "    \n",
    "    if save_path:\n",
    "        html_path = save_path.replace('.png', '.html')\n",
    "        fig.write_html(html_path)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return pca_df, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the clusters found by each algorithm in 2D\n",
    "for alg in ['kmeans', 'gmm', 'hierarchical']:\n",
    "    labels = clustering_results[alg]['labels']\n",
    "    title = f\"{alg.capitalize()} Clustering (k=3)\"\n",
    "    save_path = f\"./output/{alg}_clusters.png\"\n",
    "    pca_df, pca = visualize_clusters_2d(features_df, labels, title=title, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the best model's clusters in 3D\n",
    "best_labels = clustering_results[best_3_cluster_algorithm]['labels']\n",
    "title = f\"{best_3_cluster_algorithm.capitalize()} Clustering 3D (k=3)\"\n",
    "save_path = f\"./output/{best_3_cluster_algorithm}_clusters_3d.png\"\n",
    "pca_3d_df, pca_3d = visualize_clusters_3d(features_df, best_labels, title=title, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examine Cluster Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels to the original unscaled dataframe for interpretation\n",
    "df_with_clusters = df_unscaled.copy()\n",
    "df_with_clusters['Cluster'] = final_labels\n",
    "\n",
    "# Get the distribution of clusters\n",
    "cluster_counts = df_with_clusters['Cluster'].value_counts()\n",
    "cluster_percentages = cluster_counts / len(df_with_clusters) * 100\n",
    "\n",
    "# Print cluster distribution\n",
    "print(\"Cluster Distribution:\")\n",
    "for cluster, count in cluster_counts.iteritems():\n",
    "    print(f\"Cluster {cluster}: {count} customers ({cluster_percentages[cluster]:.2f}%)\")\n",
    "\n",
    "# Plot cluster distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = cluster_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Cluster Size Distribution', fontsize=14)\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('Number of Customers', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add data labels on top of each bar\n",
    "for i, v in enumerate(cluster_counts):\n",
    "    ax.text(i, v+5, str(v), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/cluster_distribution.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster profiles (mean of each feature for each cluster)\n",
    "cluster_profiles = df_with_clusters.groupby('Cluster').mean()\n",
    "\n",
    "# Display cluster profiles\n",
    "print(\"Cluster Profiles (Mean Values):\")\n",
    "display(cluster_profiles)\n",
    "\n",
    "# Visualize cluster profiles with a heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cluster_profiles, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Cluster Profiles (Mean Values)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/cluster_profiles_heatmap.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots for each feature by cluster\n",
    "feature_cols = [col for col in df_with_clusters.columns if col != 'customer_id' and col != 'Cluster']\n",
    "num_features = len(feature_cols)\n",
    "\n",
    "plt.figure(figsize=(15, num_features * 4))\n",
    "\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    plt.subplot(num_features, 1, i+1)\n",
    "    sns.boxplot(x='Cluster', y=feature, data=df_with_clusters, palette='viridis')\n",
    "    plt.title(f'Distribution of {feature} by Cluster', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.savefig('./output/cluster_feature_distributions.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart to visualize cluster profiles\n",
    "# First, normalize the values for better visualization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "cluster_profiles_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(cluster_profiles),\n",
    "    index=cluster_profiles.index,\n",
    "    columns=cluster_profiles.columns\n",
    ")\n",
    "\n",
    "# Create radar chart\n",
    "categories = feature_cols\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, cluster in enumerate(cluster_profiles_scaled.index):\n",
    "    values = cluster_profiles_scaled.loc[cluster].values.tolist()\n",
    "    values.append(values[0])  # Close the loop\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=values,\n",
    "        theta=categories + [categories[0]],  # Close the loop\n",
    "        fill='toself',\n",
    "        name=f'Cluster {cluster}'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 1]\n",
    "        )\n",
    "    ),\n",
    "    title='Cluster Profiles (Normalized)',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.write_html('./output/cluster_profiles_radar.html')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Clustering Results for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clustered data with customer IDs\n",
    "# Check if customer_id exists in the dataset\n",
    "if 'customer_id' in df_unscaled.columns:\n",
    "    customer_df = df_with_clusters[['customer_id', 'Cluster']]\n",
    "    customer_df.to_csv('./output/customer_clusters.csv', index=False)\n",
    "    print(f\"Saved customer cluster assignments to './output/customer_clusters.csv'\")\n",
    "    \n",
    "# Save the full dataset with cluster labels\n",
    "df_with_clusters.to_csv('./output/customer_data_with_clusters.csv', index=False)\n",
    "print(f\"Saved full dataset with cluster labels to './output/customer_data_with_clusters.csv'\")\n",
    "\n",
    "# Save cluster profiles\n",
    "cluster_profiles.to_csv('./output/cluster_profiles.csv')\n",
    "print(f\"Saved cluster profiles to './output/cluster_profiles.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initial Segment Interpretation\n",
    "\n",
    "Based on the cluster profiles and our domain knowledge of the expected customer segments, we can begin to interpret what each cluster represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative feature importance for each cluster (compared to overall mean)\n",
    "overall_mean = df_unscaled[feature_cols].mean()\n",
    "relative_importance = cluster_profiles.copy()\n",
    "\n",
    "for feature in feature_cols:\n",
    "    relative_importance[feature] = (cluster_profiles[feature] - overall_mean[feature]) / overall_mean[feature]\n",
    "\n",
    "# Display relative importance as percentages\n",
    "print(\"Relative Feature Importance (% difference from overall mean):\")\n",
    "relative_importance_pct = relative_importance.applymap(lambda x: f\"{x*100:.1f}%\")\n",
    "display(relative_importance_pct)\n",
    "\n",
    "# Visualize relative importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(relative_importance, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\n",
    "plt.title('Relative Feature Importance by Cluster', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/relative_feature_importance.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the cluster profiles, match clusters to the expected segments\n",
    "# Expected segments: Bargain Hunters, High Spenders, Window Shoppers\n",
    "\n",
    "# Create a dictionary to store interpretations\n",
    "segment_interpretations = {}\n",
    "\n",
    "# You'll need to analyze the relative importance and mean values to determine which cluster matches which segment\n",
    "# This is a simplistic example - you'll need to adapt this based on your actual cluster profiles\n",
    "\n",
    "# For each cluster, determine the matching segment based on feature values\n",
    "for cluster_id in cluster_profiles.index:\n",
    "    profile = cluster_profiles.loc[cluster_id]\n",
    "    rel_profile = relative_importance.loc[cluster_id]\n",
    "    \n",
    "    # Characteristics of Bargain Hunters:\n",
    "    # - High total_purchases\n",
    "    # - Low avg_cart_value\n",
    "    # - High discount_count\n",
    "    bargain_score = 0\n",
    "    if rel_profile['total_purchases'] > 0:\n",
    "        bargain_score += 1\n",
    "    if rel_profile['avg_cart_value'] < 0:\n",
    "        bargain_score += 1\n",
    "    discount_col = 'discount_counts' if 'discount_counts' in rel_profile else 'discount_count'\n",
    "    if rel_profile[discount_col] > 0:\n",
    "        bargain_score += 1\n",
    "    \n",
    "    # Characteristics of High Spenders:\n",
    "    # - Moderate total_purchases\n",
    "    # - High avg_cart_value\n",
    "    # - Low discount_count\n",
    "    spender_score = 0\n",
    "    if abs(rel_profile['total_purchases']) < 0.2:  # Close to average\n",
    "        spender_score += 1\n",
    "    if rel_profile['avg_cart_value'] > 0:\n",
    "        spender_score += 1\n",
    "    if rel_profile[discount_col] < 0:\n",
    "        spender_score += 1\n",
    "    \n",
    "    # Characteristics of Window Shoppers:\n",
    "    # - Low total_purchases\n",
    "    # - High total_time_spent\n",
    "    # - High product_click\n",
    "    # - Low discount_count\n",
    "    shopper_score = 0\n",
    "    if rel_profile['total_purchases'] < 0:\n",
    "        shopper_score += 1\n",
    "    if rel_profile['total_time_spent'] > 0:\n",
    "        shopper_score += 1\n",
    "    if rel_profile['product_click'] > 0:\n",
    "        shopper_score += 1\n",
    "    if rel_profile[discount_col] < 0:\n",
    "        shopper_score += 1\n",
    "    \n",
    "    # Determine the best match\n",
    "    scores = {\n",
    "        'Bargain Hunters': bargain_score,\n",
    "        'High Spenders': spender_score,\n",
    "        'Window Shoppers': shopper_score\n",
    "    }\n",
    "    best_match = max(scores, key=scores.get)\n",
    "    \n",
    "    segment_interpretations[cluster_id] = {\n",
    "        'segment': best_match,\n",
    "        'scores': scores,\n",
    "        'key_characteristics': {\n",
    "            'total_purchases': 'High' if rel_profile['total_purchases'] > 0.2 else \n",
    "                              ('Low' if rel_profile['total_purchases'] < -0.2 else 'Moderate'),\n",
    "            'avg_cart_value': 'High' if rel_profile['avg_cart_value'] > 0.2 else \n",
    "                             ('Low' if rel_profile['avg_cart_value'] < -0.2 else 'Moderate'),\n",
    "            'total_time_spent': 'High' if rel_profile['total_time_spent'] > 0.2 else \n",
    "                               ('Low' if rel_profile['total_time_spent'] < -0.2 else 'Moderate'),\n",
    "            'product_click': 'High' if rel_profile['product_click'] > 0.2 else \n",
    "                           ('Low' if rel_profile['product_click'] < -0.2 else 'Moderate'),\n",
    "            'discount_usage': 'High' if rel_profile[discount_col] > 0.2 else \n",
    "                            ('Low' if rel_profile[discount_col] < -0.2 else 'Moderate')\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Print interpretations\n",
    "for cluster_id, interpretation in segment_interpretations.items():\n",
    "    print(f\"\\nCluster {cluster_id} â†’ {interpretation['segment']}\")\n",
    "    print(\"Segment match scores:\", interpretation['scores'])\n",
    "    print(\"Key characteristics:\")\n",
    "    for characteristic, level in interpretation['key_characteristics'].items():\n",
    "        print(f\"  - {characteristic}: {level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the final segment mappings\n",
    "segment_mapping = {cluster_id: interp['segment'] for cluster_id, interp in segment_interpretations.items()}\n",
    "segment_df = pd.DataFrame({\n",
    "    'Cluster': list(segment_mapping.keys()),\n",
    "    'Segment': list(segment_mapping.values())\n",
    "})\n",
    "\n",
    "# Rename the clusters in the original dataframe\n",
    "df_with_segments = df_with_clusters.copy()\n",
    "df_with_segments['Segment'] = df_with_segments['Cluster'].map(segment_mapping)\n",
    "\n",
    "# Save the segment mapping\n",
    "segment_df.to_csv('./output/segment_mapping.csv', index=False)\n",
    "print(f\"Saved segment mapping to './output/segment_mapping.csv'\")\n",
    "\n",
    "# Save the customer data with segment labels\n",
    "df_with_segments.to_csv('./output/customer_data_with_segments.csv', index=False)\n",
    "print(f\"Saved customer data with segments to './output/customer_data_with_segments.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Found the optimal number of clusters using multiple metrics\n",
    "2. Applied different clustering algorithms (K-Means, GMM, Hierarchical, DBSCAN)\n",
    "3. Compared the performance of these algorithms\n",
    "4. Selected the best model based on evaluation metrics\n",
    "5. Visualized the identified clusters\n",
    "6. Examined cluster characteristics\n",
    "7. Interpreted the clusters as customer segments\n",
    "\n",
    "We have successfully identified three distinct customer segments that align with our domain knowledge:\n",
    "\n",
    "1. **Bargain Hunters**: Customers who make frequent purchases of low-value items and rely heavily on discounts\n",
    "2. **High Spenders**: Customers who make moderate purchases of high-value items without relying on discounts\n",
    "3. **Window Shoppers**: Customers who spend significant time browsing many products but rarely make purchases\n",
    "\n",
    "These segments provide valuable insights for targeted marketing strategies. In the next notebook, we'll perform a more in-depth analysis of these segments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
