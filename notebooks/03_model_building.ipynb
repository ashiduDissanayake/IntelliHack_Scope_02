{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation - Model Building\n",
    "\n",
    "This notebook focuses on building and evaluating clustering models to identify customer segments:\n",
    "\n",
    "1. Find the optimal number of clusters\n",
    "2. Apply different clustering algorithms\n",
    "3. Compare clustering results\n",
    "4. Select the best model\n",
    "5. Visualize the identified clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Increase default figure size\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('../output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "preprocessed_file = '../output/preprocessed_data.csv'\n",
    "df = pd.read_csv(preprocessed_file)\n",
    "\n",
    "# Also load the unscaled data for interpretation\n",
    "unscaled_file = '../output/cleaned_data_unscaled.csv'\n",
    "df_unscaled = pd.read_csv(unscaled_file)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering\n",
    "# Exclude customer_id from clustering if present\n",
    "if 'customer_id' in df.columns:\n",
    "    features_df = df.drop(columns=['customer_id'])\n",
    "else:\n",
    "    features_df = df.copy()\n",
    "\n",
    "feature_names = features_df.columns.tolist()\n",
    "print(f\"Features used for clustering: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finding the Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the optimal number of clusters\n",
    "def find_optimal_k(data, max_clusters=10):\n",
    "    # Initialize empty lists to store results\n",
    "    inertia = []\n",
    "    silhouette = []\n",
    "    calinski_harabasz = []\n",
    "    davies_bouldin = []\n",
    "    \n",
    "    # Range of clusters to try\n",
    "    K = range(2, max_clusters+1)\n",
    "    \n",
    "    for k in K:\n",
    "        # KMeans clustering\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(data)\n",
    "        \n",
    "        # Get cluster labels\n",
    "        labels = kmeans.labels_\n",
    "        \n",
    "        # Inertia (within-cluster sum-of-squares)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        \n",
    "        # Silhouette score\n",
    "        silhouette.append(silhouette_score(data, labels))\n",
    "        \n",
    "        # Calinski-Harabasz Index\n",
    "        calinski_harabasz.append(calinski_harabasz_score(data, labels))\n",
    "        \n",
    "        # Davies-Bouldin Index\n",
    "        davies_bouldin.append(davies_bouldin_score(data, labels))\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    results = {\n",
    "        'k_values': list(K),\n",
    "        'inertia': inertia,\n",
    "        'silhouette': silhouette,\n",
    "        'calinski_harabasz': calinski_harabasz,\n",
    "        'davies_bouldin': davies_bouldin\n",
    "    }\n",
    "    \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k\n",
    "results = find_optimal_k(features_df, max_clusters=10)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot Inertia (Elbow Method)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(results['k_values'], results['inertia'], 'bo-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Silhouette Score\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(results['k_values'], results['silhouette'], 'go-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score (higher is better)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Calinski-Harabasz Index\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(results['k_values'], results['calinski_harabasz'], 'ro-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "plt.title('Calinski-Harabasz Index (higher is better)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Davies-Bouldin Index\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(results['k_values'], results['davies_bouldin'], 'mo-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.title('Davies-Bouldin Index (lower is better)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/optimal_k_metrics.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use yellowbrick for elbow visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "visualizer = KElbowVisualizer(KMeans(random_state=42), k=(2, 10), timings=False)\n",
    "visualizer.fit(features_df)\n",
    "visualizer.finalize()\n",
    "plt.savefig('../output/elbow_visualizer.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use yellowbrick for silhouette visualization for k=3 (based on domain knowledge)\n",
    "plt.figure(figsize=(10, 6))\n",
    "model = KMeans(n_clusters=3, random_state=42)\n",
    "visualizer = SilhouetteVisualizer(model, colors='yellowbrick')\n",
    "visualizer.fit(features_df)\n",
    "visualizer.finalize()\n",
    "plt.title('Silhouette Plot for KMeans with 3 clusters')\n",
    "plt.savefig('../output/silhouette_k3.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimal number of clusters based on different metrics\n",
    "k_values = np.array(results['k_values'])\n",
    "silhouette_scores = np.array(results['silhouette'])\n",
    "ch_scores = np.array(results['calinski_harabasz'])\n",
    "db_scores = np.array(results['davies_bouldin'])\n",
    "\n",
    "print(f\"Optimal k based on Silhouette Score: {k_values[np.argmax(silhouette_scores)]} (score: {np.max(silhouette_scores):.4f})\")\n",
    "print(f\"Optimal k based on Calinski-Harabasz Index: {k_values[np.argmax(ch_scores)]} (score: {np.max(ch_scores):.4f})\")\n",
    "print(f\"Optimal k based on Davies-Bouldin Index: {k_values[np.argmin(db_scores)]} (score: {np.min(db_scores):.4f})\")\n",
    "print(\"\\nNote: Based on domain knowledge, we expect 3 customer segments (Bargain Hunters, High Spenders, Window Shoppers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Different Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use k=3 based on domain knowledge (we expect 3 customer segments)\n",
    "n_clusters = 3\n",
    "\n",
    "# Dictionary to store clustering results\n",
    "clustering_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. K-Means clustering\n",
    "print(\"\\nApplying K-Means clustering...\")\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "kmeans.fit(features_df)\n",
    "kmeans_labels = kmeans.labels_\n",
    "kmeans_centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "kmeans_silhouette = silhouette_score(features_df, kmeans_labels)\n",
    "kmeans_calinski = calinski_harabasz_score(features_df, kmeans_labels)\n",
    "kmeans_davies = davies_bouldin_score(features_df, kmeans_labels)\n",
    "\n",
    "print(f\"K-Means Silhouette Score: {kmeans_silhouette:.4f}\")\n",
    "print(f\"K-Means Calinski-Harabasz Index: {kmeans_calinski:.4f}\")\n",
    "print(f\"K-Means Davies-Bouldin Index: {kmeans_davies:.4f}\")\n",
    "\n",
    "# Store the results\n",
    "clustering_results['kmeans'] = {\n",
    "    'model': kmeans,\n",
    "    'labels': kmeans_labels,\n",
    "    'centroids': kmeans_centroids,\n",
    "    'silhouette': kmeans_silhouette,\n",
    "    'calinski_harabasz': kmeans_calinski,\n",
    "    'davies_bouldin': kmeans_davies\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Gaussian Mixture Model (GMM)\n",
    "print(\"\\nApplying Gaussian Mixture Model...\")\n",
    "gmm = GaussianMixture(n_components=n_clusters, random_state=42, n_init=10)\n",
    "gmm.fit(features_df)\n",
    "gmm_labels = gmm.predict(features_df)\n",
    "gmm_centroids = gmm.means_\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "gmm_silhouette = silhouette_score(features_df, gmm_labels)\n",
    "gmm_calinski = calinski_harabasz_score(features_df, gmm_labels)\n",
    "gmm_davies = davies_bouldin_score(features_df, gmm_labels)\n",
    "\n",
    "print(f\"GMM Silhouette Score: {gmm_silhouette:.4f}\")\n",
    "print(f\"GMM Calinski-Harabasz Index: {gmm_calinski:.4f}\")\n",
    "print(f\"GMM Davies-Bouldin Index: {gmm_davies:.4f}\")\n",
    "\n",
    "# Store the results\n",
    "clustering_results['gmm'] = {\n",
    "    'model': gmm,\n",
    "    'labels': gmm_labels,\n",
    "    'centroids': gmm_centroids,\n",
    "    'silhouette': gmm_silhouette,\n",
    "    'calinski_harabasz': gmm_calinski,\n",
    "    'davies_bouldin': gmm_davies\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Hierarchical Clustering\n",
    "print(\"\\nApplying Hierarchical Clustering...\")\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hierarchical_labels = hierarchical.fit_predict(features_df)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "hierarchical_silhouette = silhouette_score(features_df, hierarchical_labels)\n",
    "hierarchical_calinski = calinski_harabasz_score(features_df, hierarchical_labels)\n",
    "hierarchical_davies = davies_bouldin_score(features_df, hierarchical_labels)\n",
    "\n",
    "print(f\"Hierarchical Silhouette Score: {hierarchical_silhouette:.4f}\")\n",
    "print(f\"Hierarchical Calinski-Harabasz Index: {hierarchical_calinski:.4f}\")\n",
    "print(f\"Hierarchical Davies-Bouldin Index: {hierarchical_davies:.4f}\")\n",
    "\n",
    "# Store the results\n",
    "clustering_results['hierarchical'] = {\n",
    "    'model': hierarchical,\n",
    "    'labels': hierarchical_labels,\n",
    "    'silhouette': hierarchical_silhouette,\n",
    "    'calinski_harabasz': hierarchical_calinski,\n",
    "    'davies_bouldin': hierarchical_davies\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DBSCAN\n",
    "print(\"\\nApplying DBSCAN...\")\n",
    "# DBSCAN requires careful parameter tuning\n",
    "# We'll use a simple heuristic for eps (the maximum distance between samples)\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Determine eps value using k-distance graph (k=min_samples-1)\n",
    "k = 5  # min_samples will be k+1 = 6\n",
    "neigh = NearestNeighbors(n_neighbors=k)\n",
    "neigh.fit(features_df)\n",
    "distances, indices = neigh.kneighbors(features_df)\n",
    "distances = np.sort(distances[:, -1])\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances)\n",
    "plt.xlabel('Data Points (sorted by distance)')\n",
    "plt.ylabel(f'{k}-th Nearest Neighbor Distance')\n",
    "plt.title('K-Distance Graph for DBSCAN Parameter Selection')\n",
    "plt.grid(True)\n",
    "plt.savefig('../output/dbscan_kdistance.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Choose eps where the curve shows an \"elbow\"\n",
    "# This is a visual inspection, but you can also automate this\n",
    "eps = 0.5  # This value should be adjusted based on the k-distance graph\n",
    "min_samples = k + 1\n",
    "\n",
    "print(f\"Using eps={eps} and min_samples={min_samples} for DBSCAN\")\n",
    "\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "dbscan_labels = dbscan.fit_predict(features_df)\n",
    "\n",
    "# Count the number of clusters and noise points\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise = list(dbscan_labels).count(-1)\n",
    "print(f\"DBSCAN found {n_clusters_dbscan} clusters and {n_noise} noise points\")\n",
    "\n",
    "# Calculate evaluation metrics only if we have multiple clusters and no noise points\n",
    "# or if we can exclude noise points\n",
    "if n_clusters_dbscan > 1:\n",
    "    if -1 not in dbscan_labels:\n",
    "        dbscan_silhouette = silhouette_score(features_df, dbscan_labels)\n",
    "        dbscan_calinski = calinski_harabasz_score(features_df, dbscan_labels)\n",
    "        dbscan_davies = davies_bouldin_score(features_df, dbscan_labels)\n",
    "    else:\n",
    "        # Exclude noise points for evaluation\n",
    "        mask = dbscan_labels != -1\n",
    "        if sum(mask) > 1:  # Ensure we have at least 2 non-noise points\n",
    "            dbscan_silhouette = silhouette_score(features_df[mask], dbscan_labels[mask])\n",
    "            dbscan_calinski = calinski_harabasz_score(features_df[mask], dbscan_labels[mask])\n",
    "            dbscan_davies = davies_bouldin_score(features_df[mask], dbscan_labels[mask])\n",
    "        else:\n",
    "            dbscan_silhouette = float('nan')\n",
    "            dbscan_calinski = float('nan')\n",
    "            dbscan_davies = float('nan')\n",
    "else:\n",
    "    dbscan_silhouette = float('nan')\n",
    "    dbscan_calinski = float('nan')\n",
    "    dbscan_davies = float('nan')\n",
    "\n",
    "print(f\"DBSCAN Silhouette Score: {dbscan_silhouette:.4f}\")\n",
    "print(f\"DBSCAN Calinski-Harabasz Index: {dbscan_calinski:.4f}\")\n",
    "print(f\"DBSCAN Davies-Bouldin Index: {dbscan_davies:.4f}\")\n",
    "\n",
    "# Store the results\n",
    "clustering_results['dbscan'] = {\n",
    "    'model': dbscan,\n",
    "    'labels': dbscan_labels,\n",
    "    'n_clusters': n_clusters_dbscan,\n",
    "    'n_noise': n_noise,\n",
    "    'silhouette': dbscan_silhouette,\n",
    "    'calinski_harabasz': dbscan_calinski,\n",
    "    'davies_bouldin': dbscan_davies\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance metrics of different clustering algorithms\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Algorithm': ['K-Means', 'GMM', 'Hierarchical', 'DBSCAN'],\n",
    "    'Silhouette Score': [\n",
    "        clustering_results['kmeans']['silhouette'],\n",
    "        clustering_results['gmm']['silhouette'],\n",
    "        clustering_results['hierarchical']['silhouette'],\n",
    "        clustering_results['dbscan']['silhouette']\n",
    "    ],\n",
    "    'Calinski-Harabasz Index': [\n",
    "        clustering_results['kmeans']['calinski_harabasz'],\n",
    "        clustering_results['gmm']['calinski_harabasz'],\n",
    "        clustering_results['hierarchical']['calinski_harabasz'],\n",
    "        clustering_results['dbscan']['calinski_harabasz']\n",
    "    ],\n",
    "    'Davies-Bouldin Index': [\n",
    "        clustering_results['kmeans']['davies_bouldin'],\n",
    "        clustering_results['gmm']['davies_bouldin'],\n",
    "        clustering_results['hierarchical']['davies_bouldin'],\n",
    "        clustering_results['dbscan']['davies_bouldin']\n",
    "    ]\n",
    "})\n",
    "\n",
    "metrics_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Silhouette Score (higher is better)\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.barplot(x='Algorithm', y='Silhouette Score', data=metrics_comparison)\n",
    "plt.title('Silhouette Score Comparison (Higher is Better)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Calinski-Harabasz Index (higher is better)\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.barplot(x='Algorithm', y='Calinski-Harabasz Index', data=metrics_comparison)\n",
    "plt.title('Calinski-Harabasz Index Comparison (Higher is Better)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Davies-Bouldin Index (lower is better)\n",
    "plt.subplot(3, 1, 3)\n",
    "sns.barplot(x='Algorithm', y='Davies-Bouldin Index', data=metrics_comparison)\n",
    "plt.title('Davies-Bouldin Index Comparison (Lower is Better)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/clustering_comparison.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on Silhouette Score\n",
    "silhouette_scores = [clustering_results[alg]['silhouette'] for alg in ['kmeans', 'gmm', 'hierarchical', 'dbscan']]\n",
    "best_algorithm_idx = np.nanargmax(silhouette_scores)  # Using nanargmax to handle NaN values\n",
    "best_algorithm = ['kmeans', 'gmm', 'hierarchical', 'dbscan'][best_algorithm_idx]\n",
    "\n",
    "print(f\"Best clustering algorithm based on Silhouette Score: {best_algorithm}\")\n",
    "print(f\"Silhouette Score: {clustering_results[best_algorithm]['silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we know there are 3 customer segments, we'll use the model that performs best \n",
    "# with n_clusters=3 and has the highest Silhouette Score among K-Means, GMM, and Hierarchical\n",
    "silhouette_3_clusters = [clustering_results[alg]['silhouette'] for alg in ['kmeans', 'gmm', 'hierarchical']]\n",
    "best_3_cluster_idx = np.argmax(silhouette_3_clusters)\n",
    "best_3_cluster_algorithm = ['kmeans', 'gmm', 'hierarchical'][best_3_cluster_idx]\n",
    "\n",
    "print(f\"Best algorithm for 3 clusters: {best_3_cluster_algorithm}\")\n",
    "print(f\"Silhouette Score: {clustering_results[best_3_cluster_algorithm]['silhouette']:.4f}\")\n",
    "\n",
    "# We'll use this as our final model\n",
    "final_model = clustering_results[best_3_cluster_algorithm]['model']\n",
    "final_labels = clustering_results[best_3_cluster_algorithm]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize the Identified Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize clusters in 2D using PCA\n",
    "def visualize_clusters_2d(data, labels, title=\"Cluster Visualization\", save_path=None):\n",
    "    # Apply PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(data)\n",
    "    \n",
    "    # Create a dataframe with principal components and cluster labels\n",
    "    pca_df = pd.DataFrame(\n",
    "        data=principal_components,\n",
    "        columns=['PC1', 'PC2']\n",
    "    )\n",
    "    pca_df['Cluster'] = labels\n",
    "    \n",
    "    # Create scatter plot\n",
    